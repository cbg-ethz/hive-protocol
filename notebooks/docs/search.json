[
  {
    "objectID": "../results.html",
    "href": "../results.html",
    "title": "Posterior Diagnostics",
    "section": "",
    "text": "MCMC sampling doesn’t always work perfectly. Before trusting your results, you must verify:\n\nConvergence: Did the chains explore the posterior properly?\nEfficiency: Do we have enough effective samples?\nNo pathologies: Are there divergences or other warnings?"
  },
  {
    "objectID": "../results.html#why-diagnostics-matter",
    "href": "../results.html#why-diagnostics-matter",
    "title": "Posterior Diagnostics",
    "section": "",
    "text": "MCMC sampling doesn’t always work perfectly. Before trusting your results, you must verify:\n\nConvergence: Did the chains explore the posterior properly?\nEfficiency: Do we have enough effective samples?\nNo pathologies: Are there divergences or other warnings?"
  },
  {
    "objectID": "../results.html#setup",
    "href": "../results.html#setup",
    "title": "Posterior Diagnostics",
    "section": "Setup",
    "text": "Setup\n\nimport numpy as np\nimport polars as pl\nimport matplotlib.pyplot as plt\nimport arviz as az\n\nfrom hive_protocol.data import simulate_noisy_trajectory\nfrom hive_protocol.inference import (\n    fit_kalman_filter,\n    extract_filtered_states,\n    check_convergence,\n    compute_prediction_errors,\n    summarize_filter_performance,\n    generate_diagnostic_report,\n)"
  },
  {
    "objectID": "../results.html#generate-test-data",
    "href": "../results.html#generate-test-data",
    "title": "Posterior Diagnostics",
    "section": "Generate Test Data",
    "text": "Generate Test Data\n\n# Simulate data with known parameters\ntrue_states, observations = simulate_noisy_trajectory(\n    n_steps=80,\n    process_noise=0.3,\n    measurement_noise=0.6,\n    seed=123,\n)\n\n# Fit the model\nmodel, trace = fit_kalman_filter(\n    observations,\n    n_samples=1000,\n    n_tune=500,\n    random_seed=42,\n)\n\n# Extract filtered states\nfiltered = extract_filtered_states(trace)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [process_noise, measurement_noise, initial_state, states]\n\n\n\n\n\n\n\n\nSampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 10 seconds.\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details"
  },
  {
    "objectID": "../results.html#convergence-diagnostics",
    "href": "../results.html#convergence-diagnostics",
    "title": "Posterior Diagnostics",
    "section": "Convergence Diagnostics",
    "text": "Convergence Diagnostics\n\nAutomated Checks\nOur check_convergence function performs standard diagnostic tests:\n\nreport = check_convergence(trace)\n\nprint(\"Convergence Report:\")\nprint(f\"  Converged: {report['converged']}\")\nprint(f\"  Max R-hat: {report['rhat_max']:.4f}\")\nprint(f\"  Min ESS: {report['ess_min']}\")\nprint(f\"  Divergences: {report['n_divergences']}\")\n\nif report['warnings']:\n    print(\"\\nWarnings:\")\n    for w in report['warnings']:\n        print(f\"  - {w}\")\nelse:\n    print(\"\\nNo warnings - all diagnostics passed!\")\n\nConvergence Report:\n  Converged: False\n  Max R-hat: 1.0000\n  Min ESS: 392\n  Divergences: 0\n\nWarnings:\n  - ESS too low: 392 &lt; 400 (consider running more samples)\n\n\n\n\nUnderstanding R-hat\nR-hat (potential scale reduction factor) measures chain convergence:\n\nR-hat ≈ 1.0: Chains have converged\nR-hat &gt; 1.01: Chains may not have converged, need more samples\n\n\n# Get summary with R-hat values\nsummary = az.summary(trace)\n\nfig, ax = plt.subplots(figsize=(10, 4))\nrhats = summary[\"r_hat\"].dropna().values\nparams = summary[\"r_hat\"].dropna().index.tolist()\n\n# Truncate parameter names for readability\nshort_params = [p[:20] + \"...\" if len(p) &gt; 20 else p for p in params]\n\nax.barh(range(len(rhats)), rhats, color=\"steelblue\")\nax.axvline(x=1.01, color=\"red\", linestyle=\"--\", label=\"Threshold (1.01)\")\nax.set_yticks(range(len(rhats)))\nax.set_yticklabels(short_params, fontsize=8)\nax.set_xlabel(\"R-hat\")\nax.set_title(\"R-hat Convergence Diagnostic\")\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\nR-hat values across all parameters (should be &lt; 1.01)\n\n\n\n\n\n\nUnderstanding ESS\nEffective Sample Size (ESS) tells us how many independent samples we have:\n\nESS &gt; 400: Generally sufficient for reliable estimates\nESS &lt; 100: Estimates may be unreliable\n\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Bulk ESS\ness_bulk = summary[\"ess_bulk\"].dropna()\naxes[0].hist(ess_bulk, bins=20, color=\"steelblue\", edgecolor=\"white\")\naxes[0].axvline(x=400, color=\"red\", linestyle=\"--\", label=\"Minimum (400)\")\naxes[0].set_xlabel(\"ESS (bulk)\")\naxes[0].set_ylabel(\"Count\")\naxes[0].set_title(\"Bulk Effective Sample Size\")\naxes[0].legend()\n\n# Tail ESS\ness_tail = summary[\"ess_tail\"].dropna()\naxes[1].hist(ess_tail, bins=20, color=\"steelblue\", edgecolor=\"white\")\naxes[1].axvline(x=400, color=\"red\", linestyle=\"--\", label=\"Minimum (400)\")\naxes[1].set_xlabel(\"ESS (tail)\")\naxes[1].set_ylabel(\"Count\")\naxes[1].set_title(\"Tail Effective Sample Size\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nEffective sample size (should be &gt; 400 for bulk and tail)\n\n\n\n\n\n\nTrace Plots\nVisual inspection of trace plots reveals sampling behavior:\n\naz.plot_trace(trace, var_names=[\"process_noise\", \"measurement_noise\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\nTrace plots for noise parameters - chains should mix well\n\n\n\n\nGood trace plots show:\n\nMixing: Chains explore the same region\nStationarity: No trends over time\nNo “stuck” chains: All chains move freely"
  },
  {
    "objectID": "../results.html#filter-performance",
    "href": "../results.html#filter-performance",
    "title": "Posterior Diagnostics",
    "section": "Filter Performance",
    "text": "Filter Performance\nWhen we have ground truth (simulations), we can evaluate accuracy:\n\n# Compute prediction errors\nerrors_df = compute_prediction_errors(true_states, filtered)\n\nprint(errors_df.head(10))\n\nshape: (10, 6)\n┌──────────┬────────────┬───────────┬───────────┬───────────┬──────────────────────┐\n│ timestep ┆ true_state ┆ estimate  ┆ error     ┆ abs_error ┆ in_credible_interval │\n│ ---      ┆ ---        ┆ ---       ┆ ---       ┆ ---       ┆ ---                  │\n│ i64      ┆ f64        ┆ f64       ┆ f64       ┆ f64       ┆ bool                 │\n╞══════════╪════════════╪═══════════╪═══════════╪═══════════╪══════════════════════╡\n│ 0        ┆ -0.296736  ┆ -0.164685 ┆ 0.132051  ┆ 0.132051  ┆ true                 │\n│ 1        ┆ -0.407072  ┆ 0.037293  ┆ 0.444365  ┆ 0.444365  ┆ true                 │\n│ 2        ┆ -0.020695  ┆ 0.21745   ┆ 0.238144  ┆ 0.238144  ┆ true                 │\n│ 3        ┆ 0.037498   ┆ 0.116365  ┆ 0.078867  ┆ 0.078867  ┆ true                 │\n│ 4        ┆ 0.313567   ┆ 0.095224  ┆ -0.218343 ┆ 0.218343  ┆ true                 │\n│ 5        ┆ 0.486698   ┆ 0.138123  ┆ -0.348575 ┆ 0.348575  ┆ true                 │\n│ 6        ┆ 0.295759   ┆ 0.113494  ┆ -0.182265 ┆ 0.182265  ┆ true                 │\n│ 7        ┆ 0.458344   ┆ 0.239644  ┆ -0.218701 ┆ 0.218701  ┆ true                 │\n│ 8        ┆ 0.363366   ┆ 0.35325   ┆ -0.010115 ┆ 0.010115  ┆ true                 │\n│ 9        ┆ 0.266649   ┆ 0.376069  ┆ 0.10942   ┆ 0.10942   ┆ true                 │\n└──────────┴────────────┴───────────┴───────────┴───────────┴──────────────────────┘\n\n\n\n# Summarize performance\nperf = summarize_filter_performance(errors_df)\n\nprint(\"\\nFilter Performance Metrics:\")\nprint(f\"  RMSE (Root Mean Squared Error): {perf['rmse']:.4f}\")\nprint(f\"  MAE (Mean Absolute Error): {perf['mae']:.4f}\")\nprint(f\"  Bias (systematic over/under-estimation): {perf['bias']:.4f}\")\nprint(f\"  Coverage (% true states in 94% CI): {perf['coverage']:.1%}\")\nprint(f\"  Max Error: {perf['max_error']:.4f}\")\n\n\nFilter Performance Metrics:\n  RMSE (Root Mean Squared Error): 0.2579\n  MAE (Mean Absolute Error): 0.2041\n  Bias (systematic over/under-estimation): 0.0826\n  Coverage (% true states in 94% CI): 97.5%\n  Max Error: 0.6628\n\n\n\nVisualizing Errors\n\nfig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n\ntime = errors_df[\"timestep\"].to_numpy()\n\n# Error over time\naxes[0].plot(time, errors_df[\"error\"].to_numpy(), \"b-\", alpha=0.7)\naxes[0].axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5)\naxes[0].fill_between(time, -perf[\"rmse\"], perf[\"rmse\"], alpha=0.2, color=\"red\",\n                      label=f\"±RMSE ({perf['rmse']:.3f})\")\naxes[0].set_ylabel(\"Error (estimate - true)\")\naxes[0].set_title(\"Prediction Errors Over Time\")\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Absolute error\naxes[1].bar(time, errors_df[\"abs_error\"].to_numpy(), color=\"steelblue\", alpha=0.7)\naxes[1].axhline(y=perf[\"mae\"], color=\"red\", linestyle=\"--\",\n                label=f\"MAE ({perf['mae']:.3f})\")\naxes[1].set_xlabel(\"Time step\")\naxes[1].set_ylabel(\"Absolute Error\")\naxes[1].set_title(\"Absolute Prediction Errors\")\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nPrediction errors over time\n\n\n\n\n\n\nCoverage Analysis\nThe 94% credible interval should contain the true state ~94% of the time:\n\nin_ci = errors_df[\"in_credible_interval\"].to_numpy()\n\nfig, ax = plt.subplots(figsize=(12, 5))\n\ncolors = [\"green\" if c else \"red\" for c in in_ci]\nax.scatter(time, true_states, c=colors, s=20, alpha=0.7)\nax.fill_between(time, filtered[\"lower\"], filtered[\"upper\"],\n                alpha=0.2, color=\"blue\", label=\"94% Credible Interval\")\nax.plot(time, filtered[\"mean\"], \"b-\", linewidth=1, label=\"Filtered mean\")\n\nax.set_xlabel(\"Time step\")\nax.set_ylabel(\"Value\")\nax.set_title(f\"Coverage Analysis: {perf['coverage']:.1%} of true states in CI\")\nax.legend()\nax.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\nCredible interval coverage - green indicates true state within CI"
  },
  {
    "objectID": "../results.html#generating-reports",
    "href": "../results.html#generating-reports",
    "title": "Posterior Diagnostics",
    "section": "Generating Reports",
    "text": "Generating Reports\nFor documentation and reproducibility, generate a full diagnostic report:\n\nreport_text = generate_diagnostic_report(trace, true_states, filtered)\nprint(report_text)\n\n============================================================\nKALMAN FILTER DIAGNOSTIC REPORT\n============================================================\n\nCONVERGENCE DIAGNOSTICS\n----------------------------------------\n  Converged: NO\n  Max R-hat: 1.0000\n  Min ESS: 392\n  Divergences: 0\n\n  Warnings:\n    - ESS too low: 392 &lt; 400 (consider running more samples)\n\nFILTER PERFORMANCE\n----------------------------------------\n  RMSE: 0.2579\n  MAE: 0.2041\n  Bias: 0.0826\n  CI Coverage: 97.5%\n  Max Error: 0.6628\n\n============================================================"
  },
  {
    "objectID": "../results.html#common-problems-and-solutions",
    "href": "../results.html#common-problems-and-solutions",
    "title": "Posterior Diagnostics",
    "section": "Common Problems and Solutions",
    "text": "Common Problems and Solutions\n\nProblem 1: Low ESS\nSymptom: ESS &lt; 100 for some parameters\nSolutions: - Increase n_samples and n_tune - Reparameterize the model - Use stronger priors\n\n\nProblem 2: High R-hat\nSymptom: R-hat &gt; 1.01 for some parameters\nSolutions: - Run chains longer - Check for multimodality - Simplify the model\n\n\nProblem 3: Divergences\nSymptom: n_divergences &gt; 0\nSolutions: - Increase target_accept in sampler - Reparameterize the model (e.g., non-centered parameterization) - Check for model misspecification"
  },
  {
    "objectID": "../results.html#best-practices-checklist",
    "href": "../results.html#best-practices-checklist",
    "title": "Posterior Diagnostics",
    "section": "Best Practices Checklist",
    "text": "Best Practices Checklist\nBefore publishing results, verify:\n\nAll R-hat values &lt; 1.01\nAll ESS values &gt; 400\nNo divergent transitions\nTrace plots show good mixing\nPosterior predictive checks pass\nCredible interval coverage is reasonable\n\n\n# Automated checklist\nconvergence = check_convergence(trace)\n\nprint(\"Diagnostic Checklist:\")\nprint(f\"  [{'x' if convergence['rhat_max'] &lt; 1.01 else ' '}] R-hat &lt; 1.01\")\nprint(f\"  [{'x' if convergence['ess_min'] &gt; 400 else ' '}] ESS &gt; 400\")\nprint(f\"  [{'x' if convergence['n_divergences'] == 0 else ' '}] No divergences\")\nprint(f\"  [{'x' if perf['coverage'] &gt; 0.85 else ' '}] Coverage &gt; 85%\")\n\nall_passed = (\n    convergence['rhat_max'] &lt; 1.01 and\n    convergence['ess_min'] &gt; 400 and\n    convergence['n_divergences'] == 0 and\n    perf['coverage'] &gt; 0.85\n)\n\nif all_passed:\n    print(\"\\n All checks passed! Results are reliable.\")\nelse:\n    print(\"\\n Some checks failed. Review diagnostics before using results.\")\n\nDiagnostic Checklist:\n  [x] R-hat &lt; 1.01\n  [ ] ESS &gt; 400\n  [x] No divergences\n  [x] Coverage &gt; 85%\n\n Some checks failed. Review diagnostics before using results."
  },
  {
    "objectID": "../results.html#summary",
    "href": "../results.html#summary",
    "title": "Posterior Diagnostics",
    "section": "Summary",
    "text": "Summary\nIn this tutorial, we learned:\n\nR-hat and ESS are essential convergence diagnostics\nTrace plots provide visual confirmation of mixing\nCoverage analysis validates uncertainty quantification\nAutomated reports document analysis quality\n\nThese practices ensure your Bayesian analyses are reliable and reproducible.\n\nThis concludes the Hive-Protocol tutorial series. Happy modeling!"
  }
]